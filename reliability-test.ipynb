{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_path /scratch365/dpan/new_results/maskgen_final\n",
      "csv_path ./new_results/maskgen_final\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "import json \n",
    "import os \n",
    "from typing import Dict, Any\n",
    "\n",
    "config = {\n",
    "        \"pretrained_name\": \"google/vit-base-patch16-224\",\n",
    "        \"model_path\": \"./checkpoints/woven-dust-5/maskgen_epoch_0\",\n",
    "        \"results_path\": \"/scratch365/dpan/new_results/maskgen_final\",\n",
    "        \"csv_path\": \"./new_results/maskgen_final\",\n",
    "        \"max_samples\": 100,\n",
    "        \"dataset_split\": \"tiny\",\n",
    "        \"num_samples\": 1000,\n",
    "        \"batch_size\":1,\n",
    "        # \"auc_method\": \"prob\", # 'acc'\n",
    "        \"auc_method\": \"acc\", \n",
    "        # dummy trainer\n",
    "        \"num_steps\": 5,\n",
    "        \"mini_batch_size\": 256,\n",
    "        \"ppo_epochs\": 1,\n",
    "        \"epsilon\": 0.0,\n",
    "        \"lr\": 1e-4,\n",
    "        \"clip_param\": 0.2,\n",
    "        \"l_kl\": 1,\n",
    "        \"l_actor\": 1.0,\n",
    "        \"l_entropy\": 0.00001,\n",
    "        \"gamma\": 0.50,\n",
    "        \"tau\": 0.95,\n",
    "        \"max_epochs\": 1,\n",
    "        \"save_interval\": 50,\n",
    "        \"save_path\": \"./checkpoints\"\n",
    "}\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "print('result_path', config['results_path'])\n",
    "print(\"csv_path\", config['csv_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/afs/crc.nd.edu/user/d/dpan/wd/maskgen-explain/maskgen/vision_models/vision_maskgen.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  components = torch.load(f\"{save_path}/other_components.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted base model layers to PEFT: ['encoder.layer.0.attention.attention.query', 'encoder.layer.0.attention.attention.key', 'encoder.layer.0.attention.attention.value', 'encoder.layer.0.attention.output.dense', 'encoder.layer.0.intermediate.dense', 'encoder.layer.0.output.dense', 'encoder.layer.1.attention.attention.query', 'encoder.layer.1.attention.attention.key', 'encoder.layer.1.attention.attention.value', 'encoder.layer.1.attention.output.dense', 'encoder.layer.1.intermediate.dense', 'encoder.layer.1.output.dense', 'encoder.layer.2.attention.attention.query', 'encoder.layer.2.attention.attention.key', 'encoder.layer.2.attention.attention.value', 'encoder.layer.2.attention.output.dense', 'encoder.layer.2.intermediate.dense', 'encoder.layer.2.output.dense', 'encoder.layer.3.attention.attention.query', 'encoder.layer.3.attention.attention.key', 'encoder.layer.3.attention.attention.value', 'encoder.layer.3.attention.output.dense', 'encoder.layer.3.intermediate.dense', 'encoder.layer.3.output.dense', 'encoder.layer.4.attention.attention.query', 'encoder.layer.4.attention.attention.key', 'encoder.layer.4.attention.attention.value', 'encoder.layer.4.attention.output.dense', 'encoder.layer.4.intermediate.dense', 'encoder.layer.4.output.dense', 'encoder.layer.5.attention.attention.query', 'encoder.layer.5.attention.attention.key', 'encoder.layer.5.attention.attention.value', 'encoder.layer.5.attention.output.dense', 'encoder.layer.5.intermediate.dense', 'encoder.layer.5.output.dense', 'encoder.layer.6.attention.attention.query', 'encoder.layer.6.attention.attention.key', 'encoder.layer.6.attention.attention.value', 'encoder.layer.6.attention.output.dense', 'encoder.layer.6.intermediate.dense', 'encoder.layer.6.output.dense', 'encoder.layer.7.attention.attention.query', 'encoder.layer.7.attention.attention.key', 'encoder.layer.7.attention.attention.value', 'encoder.layer.7.attention.output.dense', 'encoder.layer.7.intermediate.dense', 'encoder.layer.7.output.dense', 'encoder.layer.8.attention.attention.query', 'encoder.layer.8.attention.attention.key', 'encoder.layer.8.attention.attention.value', 'encoder.layer.8.attention.output.dense', 'encoder.layer.8.intermediate.dense', 'encoder.layer.8.output.dense', 'encoder.layer.9.attention.attention.query', 'encoder.layer.9.attention.attention.key', 'encoder.layer.9.attention.attention.value', 'encoder.layer.9.attention.output.dense', 'encoder.layer.9.intermediate.dense', 'encoder.layer.9.output.dense', 'encoder.layer.10.attention.attention.query', 'encoder.layer.10.attention.attention.key', 'encoder.layer.10.attention.attention.value', 'encoder.layer.10.attention.output.dense', 'encoder.layer.10.intermediate.dense', 'encoder.layer.10.output.dense', 'encoder.layer.11.attention.attention.query', 'encoder.layer.11.attention.attention.key', 'encoder.layer.11.attention.attention.value', 'encoder.layer.11.attention.output.dense', 'encoder.layer.11.intermediate.dense', 'encoder.layer.11.output.dense', 'pooler.dense']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.data_utils import get_imagenet_dataloader\n",
    "from maskgen.utils.model_utils import load_exp_and_target_model\n",
    "from maskgen.trainer import PPOTrainer\n",
    "# get models \n",
    "target_model, maskgen_model, processor = load_exp_and_target_model(config, device)\n",
    "\n",
    "# get dummy trainer\n",
    "trainer = PPOTrainer(maskgen_model=maskgen_model, target_model=target_model, config=config)\n",
    "\n",
    "# get dataloader\n",
    "dataloader = get_imagenet_dataloader(split='tiny', \n",
    "                                    batch_size=config['batch_size'], \n",
    "                                    processor=processor, \n",
    "                                    shuffle=False,\n",
    "                                    num_samples=config['num_samples'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▋       | 264/1000 [00:11<00:32, 22.56it/s]/tmp/1077832.1.gpu/ipykernel_1898611/1652417463.py:12: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  stat = A / (r_std / np.sqrt(len(reward_np)))\n",
      "Processing batches: 100%|██████████| 1000/1000 [00:44<00:00, 22.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def reward_t_test(reward_np, value_np):\n",
    "    r_mean = reward_np.mean()\n",
    "    r_std = reward_np.std()\n",
    "\n",
    "    A = r_mean - value_np\n",
    "\n",
    "    stat = A / (r_std / np.sqrt(len(reward_np)))\n",
    "    # print(f\"stat: {stat}, r_mean: {r_mean}, r_std: {r_std}, value_np: {value_np}\")\n",
    "    return stat\n",
    "\n",
    "def get_heatmap(dist):\n",
    "    prob = dist.probs\n",
    "    heatmap = prob.view(1, 14, 14)  # Shape: [N, grid_size, grid_size]\n",
    "    return heatmap\n",
    "\n",
    "all_inputs = []\n",
    "all_heatmaps = []\n",
    "accept_inputs = []\n",
    "accept_heatmaps = []\n",
    "reject_inputs = []\n",
    "reject_heatmaps = []\n",
    "\n",
    "\n",
    "for idx, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Processing batches\"):\n",
    "    inputs = batch['pixel_values'].to(device)\n",
    "    with torch.no_grad():\n",
    "        predicted_class_idx = target_model(inputs).logits.argmax(-1)\n",
    "        dist, value, mu_sum_logprob = maskgen_model.get_dist_critic(inputs, predicted_class_idx.unsqueeze(1))\n",
    "        heatmap = get_heatmap(dist)\n",
    "        reward_list = []\n",
    "        for i in range(5):\n",
    "            action = trainer.get_epsilon_greedy_action(dist, 0.0)\n",
    "            _, reward = trainer.get_action_reward(inputs, action, predicted_class_idx.unsqueeze(1))\n",
    "            reward_list.append(reward)\n",
    "        reward_np = np.array([x.item() for x in reward_list])\n",
    "        value_np = value.item()\n",
    "        stat = reward_t_test(reward_np, value_np)\n",
    "        # print(stat)\n",
    "\n",
    "        # save inputs and heatmaps\n",
    "        inputs_np = inputs.cpu().numpy()\n",
    "        heatmap_np = heatmap.cpu().numpy()\n",
    "        all_inputs.append(inputs_np)\n",
    "        all_heatmaps.append(heatmap_np)\n",
    "        if np.abs(stat) < 100:\n",
    "            accept_inputs.append(inputs_np)\n",
    "            accept_heatmaps.append(heatmap_np)\n",
    "        else:\n",
    "            reject_inputs.append(inputs_np)\n",
    "            reject_heatmaps.append(heatmap_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pixel-heatmap pairs to: /scratch365/dpan/new_results/maskgen_final/pixel_heatmap_pairs.npz\n",
      "Saving pixel-heatmap pairs to: /scratch365/dpan/new_results/maskgen_final/accept/pixel_heatmap_pairs.npz\n",
      "Saving pixel-heatmap pairs to: /scratch365/dpan/new_results/maskgen_final/reject/pixel_heatmap_pairs.npz\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.save_utils import save_pixel_heatmap_pairs\n",
    "\n",
    "all_inputs_np = np.concatenate(all_inputs, axis=0)\n",
    "all_heatmaps_np = np.concatenate(all_heatmaps, axis=0)\n",
    "accept_inputs_np = np.concatenate(accept_inputs, axis=0)\n",
    "accept_heatmaps_np = np.concatenate(accept_heatmaps, axis=0)\n",
    "reject_inputs_np = np.concatenate(reject_inputs, axis=0)\n",
    "reject_heatmaps_np = np.concatenate(reject_heatmaps, axis=0)\n",
    "\n",
    "save_path = config['results_path']\n",
    "\n",
    "def save_to_file(inputs, heatmaps, save_path):\n",
    "    # ensure save path exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    new_save_path = os.path.join(save_path, 'pixel_heatmap_pairs.npz')\n",
    "    print(\"Saving pixel-heatmap pairs to:\", new_save_path)\n",
    "    save_pixel_heatmap_pairs(inputs, heatmaps, new_save_path)\n",
    "\n",
    "save_to_file(all_inputs_np, all_heatmaps_np, save_path)\n",
    "save_to_file(accept_inputs_np, accept_heatmaps_np, os.path.join(save_path, 'accept'))\n",
    "save_to_file(reject_inputs_np, reject_heatmaps_np, os.path.join(save_path, 'reject'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 3, 224, 224)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject_inputs_np.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
