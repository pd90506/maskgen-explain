{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=2, strategy='ddp_notebook', max_epochs=10)  # strategy='ddp' or 'ddp_spawn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig\n",
    "from maskgen.utils.vit_mod import ModViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datasets import load_dataset,load_metric\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# device = accelerator.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "# pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval'\n",
    "# pretrained_name = 'openai/clip-vit-base-patch32'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "# get mean and std to unnormalize the processed images\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "original_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "original_model.to('cpu')\n",
    "state_dict = original_model.state_dict()\n",
    "del original_model # remove the model to free up memory\n",
    "# change to use_mask_token = True\n",
    "pred_model = ModViTForImageClassification(config)\n",
    "# load back the parameters from state_dict\n",
    "# 为新的模型实例添加 mask_token 权重\n",
    "if 'vit.embeddings.mask_token' not in state_dict:\n",
    "    state_dict['vit.embeddings.mask_token'] = pred_model.vit.embeddings.mask_token\n",
    "\n",
    "pred_model.load_state_dict(state_dict)\n",
    "\n",
    "# set to eval mode\n",
    "pred_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# We use the same foundation model\n",
    "exp_base_model = ViTModel.from_pretrained(pretrained_name)\n",
    "exp_base_model.to('cpu')\n",
    "\n",
    "# convert to peft model and ready to use LoRA \n",
    "# 手动列出所有层的目标模块\n",
    "\n",
    "target_modules = []\n",
    "num_layers = 12  # BERT-base 有 12 层\n",
    "for i in range(num_layers):\n",
    "    target_modules.extend([\n",
    "        f\"encoder.layer.{i}.attention.attention.query\",\n",
    "        f\"encoder.layer.{i}.attention.attention.key\",\n",
    "        f\"encoder.layer.{i}.attention.attention.value\",\n",
    "        f\"encoder.layer.{i}.attention.output.dense\",\n",
    "        f\"encoder.layer.{i}.intermediate.dense\",\n",
    "        f\"encoder.layer.{i}.output.dense\"\n",
    "    ])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # 低秩矩阵的秩\n",
    "    lora_alpha=32,  # LoRA 的缩放因子\n",
    "    target_modules= target_modules,  # 目标模块\n",
    "    lora_dropout=0.1  # Dropout 概率\n",
    ")\n",
    "exp_base_model = get_peft_model(exp_base_model, lora_config)\n",
    "\n",
    "# exp_base_model = ViTModel.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from maskgen.vision_models.vision_maskgen_clip import MaskGeneratingModel\n",
    "mask_gen_model = MaskGeneratingModel(base_model=exp_base_model, pred_model=pred_model, hidden_size=config.hidden_size, num_classes=config.num_labels)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#     mask_gen_model = torch.nn.DataParallel(mask_gen_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# def load_data(seed=42): \n",
    "#     dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "#     dataset = dataset['train']\n",
    "#     splits = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "#     test_ds = splits['test']\n",
    "#     splits = splits['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "#     train_ds = splits['train']\n",
    "#     val_ds = splits['test']\n",
    "#     return train_ds, val_ds, test_ds\n",
    "\n",
    "# train_ds, val_ds, test_ds = load_data()\n",
    "# dataset = load_dataset(\"mrm8488/ImageNet1K-val\")['train']\n",
    "dataset = load_dataset('imagenet-1k', split='train', streaming=False, token=access_token, trust_remote_code=True)\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "if \"height\" in processor.size:\n",
    "    size = (processor.size[\"height\"], processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in processor.size:\n",
    "    size = processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = processor.size.get(\"longest_edge\")\n",
    "\n",
    "transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# train_ds.set_transform(preprocess)\n",
    "# test_ds.set_transform(preprocess)\n",
    "dataset.set_transform(preprocess)\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "# train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=num_workers)\n",
    "# train_dataloader = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=num_workers)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=31)\n",
    "\n",
    "# num_steps = 5\n",
    "# mini_batch_size = 256\n",
    "# ppo_epochs = 2\n",
    "# optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W814 21:12:25.020007593 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:47627 (errno: 97 - Address family not supported by protocol).\n",
      "[W814 21:12:25.153222464 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:47627 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/626 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "W0814 21:14:12.324000 22976325862464 torch/multiprocessing/spawn.py:146] Terminating process 2410798 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n    fn(i, *args)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n    self.fit_loop.run()\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n    self.advance(data_fetcher)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 250, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 190, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 268, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 167, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1306, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 153, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 238, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/adam.py\", line 205, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 321, in _training_step\n    raise RuntimeError(\nRuntimeError: Skipping the `training_step` by returning None in distributed training is not supported. It is recommended that you rewrite your training logic to avoid having to skip the step in the first place.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用 Trainer 进行训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_gen_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:189\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    187\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    188\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n    fn(i, *args)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n    self.fit_loop.run()\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n    self.advance(data_fetcher)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 250, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 190, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 268, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 167, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1306, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 153, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 238, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/optim/adam.py\", line 205, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 321, in _training_step\n    raise RuntimeError(\nRuntimeError: Skipping the `training_step` by returning None in distributed training is not supported. It is recommended that you rewrite your training logic to avoid having to skip the step in the first place.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Trainer 进行训练\n",
    "trainer.fit(mask_gen_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Wrap the model with DataParallel\n",
    "mask_gen_model = torch.nn.DataParallel(mask_gen_model)\n",
    "pred_model = torch.nn.DataParallel(pred_model)\n",
    "\n",
    "mask_gen_model = mask_gen_model.to(device)\n",
    "pred_model = pred_model.to(device)\n",
    "\n",
    "def save_model(model, model_prefix):\n",
    "    state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "    model_name = model_prefix + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".pt\"\n",
    "    model_path = os.path.join(\"trained\", model_name)\n",
    "    torch.save(state_dict, model_path)\n",
    "\n",
    "for epoch in range(5):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        # print(\"pixel_values\", pixel_values.shape)\n",
    "\n",
    "        loss_dict = train_one_batch(pred_model, pixel_values, optimizer, num_steps=num_steps, mini_batch_size=mini_batch_size, ppo_epochs=ppo_epochs)\n",
    "        desc = f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss']:.4f}, \" \\\n",
    "               f\"Actor Loss = {loss_dict['actor_loss']:.4f}, \" \\\n",
    "               f\"Critic Loss = {loss_dict['critic_loss']:.4f}, \" \\\n",
    "               f\"Entropy = {loss_dict['entropy']:.4f}, \" \\\n",
    "               f\"Returns = {loss_dict['returns']:.4f}\"\n",
    "        pbar.set_description(desc)\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"{desc}\")\n",
    "            # save the model\n",
    "            save_model(mask_gen_model, \"vision_clip_allclass_\")\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "#  Step 1: Loss = 2.6805, Actor Loss = -3.1051, Critic Loss = 11.5714, Entropy = 0.6860, Returns = 2.9787\n",
    "\n",
    "#Step 1: Loss = 0.5375, Actor Loss = -1.7771, Critic Loss = 4.6292, Entropy = 0.6860, Returns = 1.6055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.img_utils import plot_overlap_np\n",
    "from maskgen.utils import idx_to_selector\n",
    "\n",
    "# load mask_gen_model\n",
    "# mask_gen_model.load_state_dict(torch.load(\"trained/vision_mask_gen_lora_clip_20240807-125254.pt\"))\n",
    "# mask_gen_model.load_state_dict(torch.load(\"trained/vision_clip_20240812-112600.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "\n",
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "# inputs.to(device)\n",
    "\n",
    "\n",
    "# test_dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn, shuffle=True)\n",
    "# inputs = next(iter(test_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    img = inputs['pixel_values']\n",
    "    img = img.to(device)\n",
    "    pred_class = pred_model(img).logits.argmax(-1)\n",
    "    predicted_class_idx = pred_class.item()\n",
    "\n",
    "plt.title(f\"Method: MaskGen Predicted class: {pred_model.config.id2label[predicted_class_idx]}\")\n",
    "size = 14\n",
    "N = inputs['pixel_values'].shape[0]\n",
    "\n",
    "# sim = mask_gen_model.forward(pixel_values=inputs['pixel_values'])\n",
    "label = torch.tensor([predicted_class_idx]).to(device)\n",
    "dist, value = mask_gen_model.get_dist_critic(img, label.unsqueeze(1))\n",
    "sim_logits = dist.logits\n",
    "# sim = torch.sigmoid(sim_logits)\n",
    "# sim = mask_gen_model.get_attribution(inputs['pixel_values'])\n",
    "# selector = idx_to_selector(pred_class, 1000).unsqueeze(1) # [N, 1, n_classes]\n",
    "# sim = (sim * selector).sum(-1)\n",
    "# sim = sim[:,:, predicted_class_idx]\n",
    "\n",
    "heatmap = torch.sigmoid(sim_logits).reshape(N, size, size)\n",
    "\n",
    "heatmap = heatmap.squeeze(0).detach().cpu().numpy()\n",
    "img = img.squeeze(0).detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "img_int, heatmap_img = plot_overlap_np(img, heatmap, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(heatmap.min())\n",
    "print(heatmap.max())\n",
    "print(heatmap.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# save model to trained folder\n",
    "save_model(mask_gen_model, \"vision_clip_final_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# # how to check the size of the model in MB\n",
    "# import os\n",
    "# print(f\"Model size: {os.path.getsize(model_path) / 1e6:.2f} MB\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
