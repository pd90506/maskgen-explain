{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig\n",
    "from maskgen.utils import get_preprocess, collate_fn, load_imagenet\n",
    "# from maskgen.utils.img_utils import plot_overlap_np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "config = {\n",
    "        \"pretrained_name\": \"google/vit-base-patch16-224\",\n",
    "        \"results_path\": \"/scratch365/dpan/new_results/gradshap\",\n",
    "        \"max_samples\": 100,\n",
    "        \"dataset_split\": \"tiny\",\n",
    "        \"num_samples\": 1000,\n",
    "        \"batch_size\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target model loaded\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.model_utils import get_pred_model\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(config['results_path']):\n",
    "    os.makedirs(config['results_path'])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models and processor\n",
    "pretrained_name = config['pretrained_name']\n",
    "processor, target_model = get_pred_model(pretrained_name, device)\n",
    "\n",
    "print(\"target model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Siamese cat, Siamese\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.image_utils import get_image_example\n",
    "\n",
    "\n",
    "image = get_image_example(2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    img = inputs['pixel_values']\n",
    "    img = img.to(device)\n",
    "    predicted_class_idx = target_model(img).logits.argmax(-1).item()\n",
    "    secondary_class_idx = target_model(img).logits.argsort(descending=True)[0][1].item()\n",
    "\n",
    "label = predicted_class_idx\n",
    "# label = secondary_class_idx\n",
    "label = torch.tensor([label]).to(device)\n",
    "print(\"Predicted class:\", target_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 284\n",
      "Confidence: 0.964\n"
     ]
    }
   ],
   "source": [
    "from maskgen.baselines.gradshap import GradShapAnalyzer, downsample_attribution\n",
    "\n",
    "image = get_image_example(2)\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "# Initialize analyzer with wrapped model\n",
    "analyzer = GradShapAnalyzer(\n",
    "    model=target_model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get attribution for single image\n",
    "attribution_map, pred_class, confidence = analyzer.get_attribution(pixel_values)\n",
    "\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.img_utils import plot_overlap_np\n",
    "from maskgen.utils.data_utils import get_imagenet_dataloader\n",
    "\n",
    "# get dataloader\n",
    "dataloader = get_imagenet_dataloader(split='tiny', \n",
    "                                    batch_size=config['batch_size'], \n",
    "                                    processor=processor, \n",
    "                                    shuffle=False,\n",
    "                                    num_samples=config['num_samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1000/1000 [03:31<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "all_inputs = []\n",
    "all_heatmaps = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Processing batches\"):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    attribution_map, pred_class, confidence = analyzer.get_attribution(pixel_values)\n",
    "    attribution_map = downsample_attribution(attribution_map, patch_size=16)\n",
    "    attribution_map = np.expand_dims(attribution_map, axis=0)\n",
    "\n",
    "    inputs_np = pixel_values.cpu().numpy()\n",
    "    heatmap_np = attribution_map\n",
    "    all_inputs.append(inputs_np)\n",
    "    all_heatmaps.append(heatmap_np)\n",
    "\n",
    "all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "all_heatmaps = np.concatenate(all_heatmaps, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskgen.utils.save_utils import save_pixel_heatmap_pairs\n",
    "\n",
    "save_path = config['results_path']\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "save_path = os.path.join(save_path, 'pixel_heatmap_pairs.npz')\n",
    "save_pixel_heatmap_pairs(all_inputs, all_heatmaps, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
