{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig\n",
    "from maskgen.utils import get_preprocess, collate_fn, load_imagenet\n",
    "# from maskgen.utils.img_utils import plot_overlap_np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "config = {\n",
    "        \"pretrained_name\": \"google/vit-base-patch16-224\",\n",
    "        \"results_path\": \"/scratch365/dpan/new_results/gradshap\",\n",
    "        \"max_samples\": 100,\n",
    "        \"dataset_split\": \"tiny\",\n",
    "        \"num_samples\": 1000,\n",
    "        \"batch_size\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target model loaded\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.model_utils import get_pred_model\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(config['results_path']):\n",
    "    os.makedirs(config['results_path'])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models and processor\n",
    "pretrained_name = config['pretrained_name']\n",
    "processor, target_model = get_pred_model(pretrained_name, device)\n",
    "\n",
    "print(\"target model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Siamese cat, Siamese\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.image_utils import get_image_example\n",
    "\n",
    "\n",
    "image = get_image_example(2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    img = inputs['pixel_values']\n",
    "    img = img.to(device)\n",
    "    predicted_class_idx = target_model(img).logits.argmax(-1).item()\n",
    "    secondary_class_idx = target_model(img).logits.argsort(descending=True)[0][1].item()\n",
    "\n",
    "label = predicted_class_idx\n",
    "# label = secondary_class_idx\n",
    "label = torch.tensor([label]).to(device)\n",
    "print(\"Predicted class:\", target_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 284\n",
      "Confidence: 0.964\n"
     ]
    }
   ],
   "source": [
    "from maskgen.baselines.gradshap import GradShapAnalyzer, downsample_attribution\n",
    "\n",
    "image = get_image_example(2)\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "# Initialize analyzer with wrapped model\n",
    "analyzer = GradShapAnalyzer(\n",
    "    model=target_model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get attribution for single image\n",
    "attribution_map, pred_class, confidence = analyzer.get_attribution(pixel_values)\n",
    "\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from maskgen.utils.img_utils import plot_overlap_np\n",
    "from maskgen.utils.data_utils import get_imagenet_dataloader\n",
    "\n",
    "# get dataloader\n",
    "dataloader = get_imagenet_dataloader(split='tiny', \n",
    "                                    batch_size=config['batch_size'], \n",
    "                                    processor=processor, \n",
    "                                    shuffle=False,\n",
    "                                    num_samples=config['num_samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|‚ñè         | 13/1000 [00:03<04:50,  3.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m attribution_map, pred_class, confidence \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m attribution_map \u001b[38;5;241m=\u001b[39m downsample_attribution(attribution_map, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      9\u001b[0m attribution_map \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(attribution_map, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/wd/maskgen-explain/maskgen/baselines/gradshap.py:114\u001b[0m, in \u001b[0;36mGradShapAnalyzer.get_attribution\u001b[0;34m(self, pixel_values, target_class)\u001b[0m\n\u001b[1;32m    105\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_shap\u001b[38;5;241m.\u001b[39mattribute(\n\u001b[1;32m    106\u001b[0m     pixel_values,\n\u001b[1;32m    107\u001b[0m     baselines\u001b[38;5;241m=\u001b[39mbaseline,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     stdevs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdevs\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Convert to numpy array\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m attribution_map \u001b[38;5;241m=\u001b[39m \u001b[43mattribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribution_map, predicted_class, confidence\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_inputs = []\n",
    "all_heatmaps = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Processing batches\"):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    attribution_map, pred_class, confidence = analyzer.get_attribution(pixel_values)\n",
    "    attribution_map = downsample_attribution(attribution_map, patch_size=16)\n",
    "    attribution_map = np.expand_dims(attribution_map, axis=0)\n",
    "\n",
    "    inputs_np = pixel_values.cpu().numpy()\n",
    "    heatmap_np = attribution_map\n",
    "    all_inputs.append(inputs_np)\n",
    "    all_heatmaps.append(heatmap_np)\n",
    "\n",
    "all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "all_heatmaps = np.concatenate(all_heatmaps, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.9054529e-03,  1.5604202e-04,  1.4694926e-04,  1.7749018e-04,\n",
       "          2.3441883e-03,  1.3592049e-03,  4.5233557e-04, -2.4134452e-03,\n",
       "          4.8402196e-04, -9.8332076e-04,  2.9184911e-03,  2.8672721e-03,\n",
       "          3.3591727e-03,  2.2763824e-03],\n",
       "        [ 1.5924835e-03,  6.2206975e-04,  3.7740611e-03,  1.1426775e-04,\n",
       "          1.1477394e-03,  3.1292683e-04,  3.0242073e-04,  2.6950052e-03,\n",
       "          4.5743445e-04,  1.9871923e-03,  3.2793727e-05,  1.2660279e-03,\n",
       "         -1.2142060e-03,  2.5763519e-03],\n",
       "        [ 4.2918284e-04, -3.7089051e-04,  1.0725774e-03, -8.8884583e-04,\n",
       "         -2.0371848e-03,  7.0266845e-04,  2.4799481e-03,  1.1165568e-04,\n",
       "          4.1146944e-03,  8.2621612e-03,  2.0356448e-03,  6.3023518e-04,\n",
       "         -1.0789346e-03,  1.4509135e-03],\n",
       "        [-2.5765563e-05, -6.9733697e-04, -5.3256151e-04, -9.2111941e-04,\n",
       "         -2.0169171e-03, -1.6627368e-03, -1.2189907e-03,  4.2291026e-04,\n",
       "         -2.1550287e-03,  1.3859791e-03,  1.0740218e-03,  7.2542857e-04,\n",
       "          4.4529053e-04,  3.2759369e-03],\n",
       "        [-2.2269844e-05,  7.0863171e-05,  1.1122356e-03, -1.2320722e-03,\n",
       "          5.0323270e-04, -2.9918808e-03, -1.6778302e-03, -3.1136428e-03,\n",
       "          7.8583369e-04,  8.8419730e-04,  2.9030826e-04, -3.4046797e-03,\n",
       "         -1.9547124e-03,  1.7832501e-03],\n",
       "        [-3.4661905e-04, -1.0034733e-03,  2.5482618e-04, -5.0693750e-04,\n",
       "          1.6783293e-03,  5.7909661e-04, -8.3069899e-04,  1.1399982e-04,\n",
       "         -1.3312092e-03, -2.2619180e-03, -3.2374896e-03, -1.7335097e-03,\n",
       "          3.0742842e-05,  1.6365439e-04],\n",
       "        [ 4.5763998e-04, -6.8434875e-04,  2.8336868e-03,  3.2099808e-04,\n",
       "         -7.8378245e-04,  1.4682609e-03, -2.8637582e-03, -7.3820434e-04,\n",
       "          1.1396813e-03,  3.8378523e-04, -2.9318037e-03,  1.9290163e-03,\n",
       "          3.0922145e-04, -3.8300018e-04],\n",
       "        [-9.6614478e-04, -1.3990712e-03,  5.0136301e-04,  1.7525366e-03,\n",
       "         -8.8115886e-04, -1.8482163e-03,  5.7960814e-04,  2.4920180e-03,\n",
       "          8.5154444e-04, -1.4857031e-04, -4.5535091e-04, -1.0225332e-03,\n",
       "         -1.0589326e-03, -2.4096724e-03],\n",
       "        [-1.1177021e-03, -1.1107064e-03, -9.9363737e-04,  1.5672373e-04,\n",
       "          3.0940096e-04, -1.6273640e-04,  2.8074419e-03, -2.7178274e-03,\n",
       "          1.4530274e-04,  2.0935977e-04,  6.0729333e-05,  1.2191200e-03,\n",
       "          3.2439825e-04, -8.4141822e-04],\n",
       "        [-8.7668258e-04, -1.2380226e-03,  2.3440742e-03,  5.4589682e-04,\n",
       "          3.1051296e-04, -8.4350520e-04,  5.4741977e-04, -1.7069203e-03,\n",
       "          1.6917347e-03, -8.7644847e-04,  9.8789949e-04, -1.2763345e-04,\n",
       "         -4.9103337e-04,  5.3531269e-04],\n",
       "        [ 2.5115856e-03, -1.1434313e-03, -3.7535961e-04,  2.5006256e-04,\n",
       "         -6.4168067e-05,  4.3329259e-05,  1.2460096e-03,  1.0474310e-03,\n",
       "         -6.9317536e-04,  1.0769094e-03, -2.5446936e-03, -1.0344965e-03,\n",
       "         -3.3333874e-04,  8.3557831e-04],\n",
       "        [ 8.9495804e-04, -2.9917422e-04,  1.1805333e-03, -9.4578922e-04,\n",
       "          4.2488833e-04, -6.1571761e-04, -3.9355410e-04,  2.0630972e-04,\n",
       "          1.7791506e-03, -4.6972634e-04, -7.9761568e-04, -7.4577221e-04,\n",
       "          1.5155203e-04,  1.7150259e-04],\n",
       "        [-2.9257161e-04,  4.3055223e-04, -1.7310685e-04, -2.4575344e-04,\n",
       "          1.6655748e-03, -2.3802277e-06,  2.8133253e-04,  2.3170426e-03,\n",
       "         -4.5628956e-04, -1.9338905e-03, -8.9923787e-04, -1.2065205e-03,\n",
       "          2.9540954e-03, -1.2176433e-03],\n",
       "        [ 8.0561708e-04, -3.7403766e-04,  2.1172405e-04,  1.0837070e-04,\n",
       "         -3.7665511e-04, -7.2589377e-04, -4.6156137e-04, -9.0075389e-04,\n",
       "         -2.3652322e-03, -1.5459163e-04, -9.6985209e-04,  1.0179074e-03,\n",
       "         -9.6760428e-04,  9.5472706e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskgen.utils.save_utils import save_pixel_heatmap_pairs\n",
    "\n",
    "save_path = config['results_path']\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "save_path = os.path.join(save_path, 'pixel_heatmap_pairs.npz')\n",
    "save_pixel_heatmap_pairs(all_inputs, all_heatmaps, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
