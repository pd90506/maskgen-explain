{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datasets import load_dataset,load_metric\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://images.cocodataset.org/val2017/000000039769.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m      7\u001b[0m pretrained_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval'\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# pretrained_name = 'openai/clip-vit-base-patch32'\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "# pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval'\n",
    "# pretrained_name = 'openai/clip-vit-base-patch32'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "# get mean and std to unnormalize the processed images\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "# set to eval mode\n",
    "pred_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "    logits = outputs.logits\n",
    "    # model predicts one of the 1000 ImageNet classes\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from maskgen.models.vision_maskgen_model10 import MaskGeneratingModel\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(pred_model, hidden_size=config.hidden_size, num_classes=config.num_labels)\n",
    "mask_gen_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def load_data(seed=42): \n",
    "    dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "    dataset = dataset['train']\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    test_ds = splits['test']\n",
    "    splits = splits['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "    train_ds = splits['train']\n",
    "    val_ds = splits['test']\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = load_data()\n",
    "dataset = load_dataset(\"mrm8488/ImageNet1K-val\")['train']\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "if \"height\" in processor.size:\n",
    "    size = (processor.size[\"height\"], processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in processor.size:\n",
    "    size = processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = processor.size.get(\"longest_edge\")\n",
    "\n",
    "transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_ds.set_transform(preprocess)\n",
    "test_ds.set_transform(preprocess)\n",
    "dataset.set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params to be optimized: \n",
      "['similarity_measure.logit_scale', 'similarity_measure.pred_map.input_layer.weight', 'similarity_measure.pred_map.input_layer.bias', 'similarity_measure.pred_map.layers.0.0.weight', 'similarity_measure.pred_map.layers.0.0.bias', 'similarity_measure.pred_map.layers.0.3.weight', 'similarity_measure.pred_map.layers.0.3.bias', 'similarity_measure.pred_map.layers.0.6.weight', 'similarity_measure.pred_map.layers.0.6.bias', 'similarity_measure.pred_map.layers.1.0.weight', 'similarity_measure.pred_map.layers.1.0.bias', 'similarity_measure.pred_map.layers.1.3.weight', 'similarity_measure.pred_map.layers.1.3.bias', 'similarity_measure.pred_map.layers.1.6.weight', 'similarity_measure.pred_map.layers.1.6.bias', 'similarity_measure.pred_map.output_layer.weight', 'similarity_measure.pred_map.output_layer.bias', 'similarity_measure.explain_map.input_layer.weight', 'similarity_measure.explain_map.input_layer.bias', 'similarity_measure.explain_map.layers.0.0.weight', 'similarity_measure.explain_map.layers.0.0.bias', 'similarity_measure.explain_map.layers.0.3.weight', 'similarity_measure.explain_map.layers.0.3.bias', 'similarity_measure.explain_map.layers.0.6.weight', 'similarity_measure.explain_map.layers.0.6.bias', 'similarity_measure.explain_map.layers.1.0.weight', 'similarity_measure.explain_map.layers.1.0.bias', 'similarity_measure.explain_map.layers.1.3.weight', 'similarity_measure.explain_map.layers.1.3.bias', 'similarity_measure.explain_map.layers.1.6.weight', 'similarity_measure.explain_map.layers.1.6.bias', 'similarity_measure.explain_map.output_layer.weight', 'similarity_measure.explain_map.output_layer.bias']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=num_workers)\n",
    "# train_dataloader = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=num_workers)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "n_steps = 5\n",
    "n_samples = 5\n",
    "\n",
    "params_to_optimize = [name for name, param in mask_gen_model.named_parameters() if param.requires_grad]\n",
    "print(\"params to be optimized: \")\n",
    "print(params_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = 0.4416, Reward Loss = -0.4419, Mask Loss = 0.8835 mask_mean = 0.5033 prob_mean = 0.5710 :   0%|          | 0/196 [00:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.2406, Reward Loss = -0.5283, Mask Loss = 0.7689 mask_mean = 0.4456 prob_mean = 0.5551 :   5%|▌         | 10/196 [04:51<1:16:21, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 19: Loss = 0.1724, Reward Loss = -0.5403, Mask Loss = 0.7127 mask_mean = 0.4184 prob_mean = 0.5536 :  10%|▉         | 19/196 [08:33<1:28:10, 29.89s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "params_to_optimize = [param for param in mask_gen_model.parameters() if param.requires_grad]\n",
    "# optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        loss_dict = mask_gen_model.train_one_batch(pixel_values, optimizer=optimizer, n_steps=n_steps, n_samples=n_samples)\n",
    "        # scheduler.step()\n",
    "        pbar.set_description(f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss'].item():.4f}, \" \n",
    "                             f\"Reward Loss = {loss_dict['reward_loss'].item():.4f}, \"\n",
    "                            #  f\"Regret Loss = {loss_dict['regret_loss'].item():.4f}, \"\n",
    "                             f\"Mask Loss = {loss_dict['mask_loss'].item():.4f} \"\n",
    "                            #  f\"alt_mask_loss = {loss_dict['alt_mask_loss'].item():.4f} \"\n",
    "                             f\"mask_mean = {loss_dict['mask_mean'].item():.4f} \"\n",
    "                             f\"prob_mean = {loss_dict['prob_mean'].item():.4f} \"\n",
    "                             )\n",
    "        if idx % 10 == 0:\n",
    "            print()\n",
    "        if (idx) % 10 == 0:\n",
    "            \n",
    "            torch.save(mask_gen_model.state_dict(), f'mask_gen_model/mask_gen_model_{epoch}_{idx}.pth') \n",
    "\n",
    "\n",
    "\n",
    "torch.save(mask_gen_model.state_dict(), f'mask_gen_model/mask_gen_model_final_{epoch}_{idx}.pth') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
