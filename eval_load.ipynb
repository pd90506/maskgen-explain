{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datasets import load_dataset,load_metric\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: golden retriever\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "# pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval'\n",
    "# pretrained_name = 'openai/clip-vit-base-patch32'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "# get mean and std to unnormalize the processed images\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "# set to eval mode\n",
    "pred_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "    logits = outputs.logits\n",
    "    # model predicts one of the 1000 ImageNet classes\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(seed=42): \n",
    "    dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "    dataset = dataset['train']\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    test_ds = splits['test']\n",
    "    splits = splits['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "    train_ds = splits['train']\n",
    "    val_ds = splits['test']\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, _, test_ds = load_data()\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "if \"height\" in processor.size:\n",
    "    size = (processor.size[\"height\"], processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in processor.size:\n",
    "    size = processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = processor.size.get(\"longest_edge\")\n",
    "\n",
    "transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "test_ds.set_transform(preprocess)\n",
    "train_ds.set_transform(preprocess)\n",
    "\n",
    "# batch size is limited to 2, because n_steps could could huge memory consumption\n",
    "batch_size = 1000\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "# test_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8940000534057617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8080000281333923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734000027179718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6610000133514404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5640000104904175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.453000009059906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3370000123977661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19200000166893005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:13, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07500000298023224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "hm_path = 'results/maskgen-vit/heatmap-5000.npy'\n",
    "# hm_path = 'results/rise-vit/heatmap-5000.npy'\n",
    "# hm_path = 'results/maskgen_model3-vit/heatmap-5000.npy'\n",
    "# hm_path = 'results/attvis-vit/heatmap-5000.npy'\n",
    "# hm_path = 'results/gradcam-vit/heatmap-5000.npy'\n",
    "# hm_path = 'random'\n",
    "\n",
    "def obtain_masks_on_topk(attribution, topk, mode='positive'):\n",
    "    \"\"\" \n",
    "    attribution: [N, H_a, W_a]\n",
    "    \"\"\"\n",
    "    H_a, W_a = attribution.shape[-2:]\n",
    "    attribution = attribution.reshape(-1, H_a * W_a) # [N, H_a*W_a]\n",
    "    attribution_perturb = attribution + 1e-6*torch.randn_like(attribution) # to avoid equal attributions (typically all zeros or all ones)\n",
    "    \n",
    "    attribution_size = H_a * W_a\n",
    "    topk_scaled = int(topk * attribution_size / 100)\n",
    "    if mode == 'positive':\n",
    "        a, _ = torch.topk(attribution_perturb, k=topk_scaled, dim=-1)\n",
    "        a = a[:, -1].unsqueeze(-1)\n",
    "        mask = (attribution_perturb <= a).float()\n",
    "    elif mode == 'negative':\n",
    "        a, _ = torch.topk(attribution_perturb, k=topk_scaled, dim=-1, largest=False)\n",
    "        a = a[:, -1].unsqueeze(-1)\n",
    "        mask = (attribution_perturb >= a).float()\n",
    "        # a, _ = torch.topk(attribution_perturb, k=topk_scaled, dim=-1)\n",
    "        # a = a[:, -1].unsqueeze(-1)\n",
    "        # mask = (attribution_perturb >= a).float()\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Enter game mode either as positive or negative.')\n",
    "    return mask.reshape(-1, H_a, W_a) # [N, H_a*W_a]\n",
    "\n",
    "\n",
    "def obtain_masked_input_on_topk(x, attribution, topk, mode='positive'):\n",
    "    \"\"\" \n",
    "    x: [N, C, H, W]\n",
    "    attribution: [N, H_a, W_a]\n",
    "    \"\"\"\n",
    "    mask = obtain_masks_on_topk(attribution, topk, mode)\n",
    "    mask = mask.unsqueeze(1) # [N, 1, H_a, W_a]\n",
    "    mask = F.interpolate(mask, size=x.shape[-2:], mode='nearest')\n",
    "\n",
    "    masked_input = x * mask\n",
    "\n",
    "    # mean_pixel = masked_input.sum(dim=(-1, -2), keepdim=True) / mask.sum(dim=(-1, -2), keepdim=True)\n",
    "    mean_pixel = x.mean(dim=(-1, -2), keepdim=True)\n",
    "    masked_input = masked_input + (1 - mask) * mean_pixel\n",
    "\n",
    "    return masked_input\n",
    "\n",
    "def load_heatmap(path='results/maskgen-vit/heatmap-5000.npy', batch_size=1000):\n",
    "    if path == 'random':\n",
    "        heatmap = np.random.rand(5000, 1, 14, 14)\n",
    "        heatmap = torch.tensor(heatmap)\n",
    "    else:\n",
    "        heatmap = np.load(path)\n",
    "        heatmap = torch.tensor(heatmap) # [N, 1, 14, 14]\n",
    "    batches = torch.split(heatmap, batch_size, dim=0)\n",
    "    return batches\n",
    "\n",
    "\n",
    "topk = 20\n",
    "# divide heatmap into batches of size batch_size\n",
    "heatmap_batches = load_heatmap(path=hm_path, batch_size=1000)\n",
    "\n",
    "total_acc = []\n",
    "for topk in [10, 20, 30,  40,  50,  60,  70, 80,  90]:\n",
    "    for idx, data in tqdm(enumerate(zip(test_dataloader, heatmap_batches))):\n",
    "        pixel_values = data[0]['pixel_values'].to(device) # [N, C, H, W]\n",
    "        attribution = data[1].to(device) # [N, 1, 14, 14]\n",
    "        with torch.no_grad():\n",
    "            pseudo_label = pred_model(pixel_values).logits.argmax(-1).view(-1)\n",
    "            masked_input = obtain_masked_input_on_topk(pixel_values, attribution, topk, mode='positive')\n",
    "            logits = pred_model(masked_input).logits\n",
    "            preds = logits.argmax(-1).view(-1)\n",
    "            acc = (pseudo_label == preds).float().mean().item()\n",
    "            print(acc)\n",
    "            total_acc.append(acc)\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5242222398519516"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_acc) / len(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
