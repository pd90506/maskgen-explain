# Refactored vision mask gen model. 
# Splited the model structure and the training logic 
# The training logic is implemented using the torch lightning module.

from transformers import ViTModel
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
import torch
import torch.nn as nn


class MaskGeneratingModel(nn.Module):
    def __init__(self, base_model_name: str, hidden_size: int, num_classes: int, freeze_base: bool = True):
        super().__init__()

        # Load the base ViT model
        self.base_model = ViTModel.from_pretrained(base_model_name)

        # Actor network for policy generation
        self.actor = nn.Sequential(
            nn.Linear(self.base_model.config.hidden_size, hidden_size),
            nn.Linear(hidden_size, num_classes),
        )

        # Critic network for value estimation
        self.critic = nn.Sequential(
            nn.Linear(self.base_model.config.hidden_size, hidden_size),
            nn.Linear(hidden_size, 1),  # Outputs a single value
        )

        # Freeze the base model if required
        if freeze_base:
            for param in self.base_model.parameters():
                param.requires_grad = False

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor):
        """
        Forward pass through the model.

        Args:
            pixel_values (torch.Tensor): Input image tensor of shape (N, C, H, W).
            labels (torch.Tensor): Ground truth labels of shape (N,).

        Returns:
            dist (torch.distributions.Bernoulli): Bernoulli distribution for mask generation.
            value (torch.Tensor): Critic value of shape (N,).
            mu_logits (torch.Tensor): Logits generated by the actor of shape (N, L, num_classes).
        """
        # Forward pass through the base model
        base_output = self.base_model(pixel_values)
        hidden_states = base_output.last_hidden_state  # Shape: [N, L, D]

        # Actor outputs logits for all tokens
        mu_logits = self.actor(hidden_states)  # Shape: [N, L, num_classes]

        # Critic value estimation using the [CLS] token
        pooled_output = hidden_states[:, 0, :]  # [CLS] token representation
        value = self.critic(pooled_output).squeeze(-1)  # Shape: [N]

        return mu_logits, value

    @torch.no_grad()
    def attribute_img(self, pixel_values: torch.Tensor, labels: torch.Tensor, image_size: int = 224, patch_size: int = 16):
        """
        Generate attribution heatmaps for input images.

        Args:
            pixel_values (torch.Tensor): Input image tensor of shape (N, C, H, W).
            labels (torch.Tensor): Ground truth labels of shape (N,).
            image_size (int, optional): Size of the input image (H = W = image_size). Defaults to 224.
            patch_size (int, optional): Size of the patch in ViT. Defaults to 16.

        Returns:
            heatmap (torch.Tensor): Attribution heatmap of shape (N, H/patch_size, W/patch_size).
        """
        dist, _, _ = self.get_dist_critic(pixel_values, labels)
        sim_probs = dist.probs  # Shape: [N, L]

        grid_size = image_size // patch_size
        heatmap = sim_probs.view(-1, grid_size, grid_size)  # Shape: [N, grid_size, grid_size]
        return heatmap



def wrap_as_peft_model(model: MaskGeneratingModel, r: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.1):
    """
    Wraps the entire MaskGeneratingModel as a PEFT model for parameter-efficient fine-tuning.

    Args:
        model (MaskGeneratingModel): The base model to wrap.
        r (int, optional): LoRA rank. Defaults to 8.
        lora_alpha (int, optional): LoRA alpha. Defaults to 32.
        lora_dropout (float, optional): LoRA dropout rate. Defaults to 0.1.

    Returns:
        PeftModel: The wrapped PEFT model.
    """
    lora_config = LoraConfig(
        task_type=TaskType.FEATURE_EXTRACTION,
        inference_mode=False,
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
    )

    # Wrap the entire model with PEFT
    return get_peft_model(model, lora_config)





def save_model_example():
    # Initialize using the base model
    model = MaskGeneratingModel(
        base_model_name="google/vit-base-patch16-224",
        hidden_size=256,
        num_classes=10,
        freeze_base=True,
    )

    # Wrap the model with PEFT
    peft_model = wrap_as_peft_model(model)

    # Save the PEFT-wrapped model
    peft_model.save_pretrained("path_to_save_model")

def load_model_example():
    # Load the with original base model
    model = MaskGeneratingModel(
        base_model_name="google/vit-base-patch16-224",
        hidden_size=256,
        num_classes=10,
        freeze_base=True,
    )

    # Load the PEFT-wrapped model with saved parameters
    peft_model = PeftModel.from_pretrained(model, "path_to_save_model")








