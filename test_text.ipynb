{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
    "from datasets import load_dataset,load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "{'input_ids': tensor([[ 101, 1045, 2066, 2023, 3185,  999,  102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "texts = \"I like this movie!\"\n",
    "\n",
    "pretrained_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "pred_config = DistilBertConfig.from_pretrained(pretrained_name)\n",
    "pred_tokenizer = DistilBertTokenizer.from_pretrained(pretrained_name)\n",
    "pred_model = DistilBertForSequenceClassification.from_pretrained(pretrained_name).to(device)\n",
    "\n",
    "inputs = pred_tokenizer(texts, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    inputs = {key:val.to(device) for key,val in inputs.items()}\n",
    "    logits = pred_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(pred_model.config.id2label[predicted_class_id])\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "explain_pretrained_name = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "explain_tokenizer = CLIPTokenizer.from_pretrained(explain_pretrained_name)\n",
    "explain_model = CLIPTextModel.from_pretrained(explain_pretrained_name).to(device)\n",
    "explain_config = explain_model.config\n",
    "\n",
    "explain_inputs = explain_tokenizer(texts, return_tensors=\"pt\")\n",
    "\n",
    "# print(explain_inputs)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     explain_inputs = {key:val.to(device) for key,val in explain_inputs.items()}\n",
    "#     explain_logits = explain_model(**explain_inputs).logits\n",
    "print(explain_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 768 2\n"
     ]
    }
   ],
   "source": [
    "pred_hidden_dim = pred_model.config.dim\n",
    "num_labels = pred_model.config.num_labels\n",
    "explain_hidden_dim = explain_config.projection_dim\n",
    "# clip_model.config\n",
    "print(pred_hidden_dim, explain_hidden_dim, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from maskgen.utils import idx_to_selector\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from maskgen.models import MLP\n",
    "import math\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class SimilarityMeasure(nn.Module):\n",
    "    def __init__(self, pred_hidden_size, explain_hidden_size, embed_size=512):\n",
    "        super(SimilarityMeasure, self).__init__()\n",
    "\n",
    "        self.pred_map = MLP(pred_hidden_size, 128, embed_size, num_blocks=2, bottleneck_dim=64)\n",
    "        self.explain_map = MLP(explain_hidden_size, 128, embed_size, num_blocks=2, bottleneck_dim=64)\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(1.0))\n",
    "    \n",
    "    def forward(self, pred_feature, explain_features):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape [N, pred_hidden_size].\n",
    "            k (torch.Tensor): Key tensor of shape [N, L, explain_hidden_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Similarity tensor of shape [N, L].\n",
    "        \"\"\"\n",
    "        pred_feature = F.normalize(self.pred_map(pred_feature), p=2, dim=-1).unsqueeze(1)  # [N, 1, embed_size]\n",
    "        explain_features = F.normalize(self.explain_map(explain_features), p=2, dim=-1)  # [N, L, embed_size]\n",
    "\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        similarity = torch.matmul(explain_features, pred_feature.transpose(-1, -2)).squeeze(-1) * logit_scale  # [N, L]\n",
    "\n",
    "        return similarity  # [N, L]\n",
    "\n",
    "\n",
    "class MaskGeneratingModel(nn.Module):\n",
    "    def __init__(self, pred_model: nn.Module, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.explain_model = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "        self.pred_model = pred_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        explain_hidden_size = self.explain_model.config.projection_dim\n",
    "        pred_hidden_size = hidden_size\n",
    "        self.similarity_measure = SimilarityMeasure(pred_hidden_size, explain_hidden_size)\n",
    "\n",
    "        self.mask_token = 103\n",
    "\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "\n",
    "        self.freeze_params()\n",
    "\n",
    "\n",
    "    def freeze_params(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of the ViT and prediction model.\n",
    "\n",
    "        This method sets the `requires_grad` attribute of all parameters in the ViT and prediction model to False,\n",
    "        effectively freezing them and preventing them from being updated during training.\n",
    "        \"\"\"\n",
    "        for param in self.explain_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.pred_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def get_interpretable_text_features(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Extract interpretable features using the text part of CLIP model.\n",
    "\n",
    "        Args:\n",
    "            x: An image tensor of shape [N, C, H, W].\n",
    "\n",
    "        Returns:\n",
    "            Interpretable features of shape [N, L, d].\n",
    "        \"\"\"\n",
    "        # get the output of the ViT model\n",
    "        print(input_ids, attention_mask)\n",
    "        output = self.explain_model(input_ids, attention_mask)\n",
    "        # get the last hidden state, exclude the cls token\n",
    "        hidden_states = output['last_hidden_state'][:, 1:-1, :]  # [N, L, d], the first and last tokens are [CLS] and [SEP]\n",
    "        return hidden_states\n",
    "    \n",
    "    def get_original_text_feature(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, predicted_class_selector: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Extract the original feature using the original prediction model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: The input tensor of shape [N, L].\n",
    "\n",
    "        Returns:\n",
    "            Original feature of shape [N, d].\n",
    "        \"\"\"\n",
    "        # get the output of the prediction model\n",
    "        output = self.pred_model.distilbert(input_ids, attention_mask)\n",
    "        # get the first hidden state, which corresponds to the cls token\n",
    "        hidden_state = output[0] # [N, d]\n",
    "        pooled_output = hidden_state[:, 0, :] # [N, d]\n",
    "        pooled_output = self.pred_model.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "\n",
    "        W = self.pred_model.classifier.weight # [n_classes, d]\n",
    "        original_feature = pooled_output.unsqueeze(1) * W.unsqueeze(0)   # [N, n_classes, d]\n",
    "        original_feature = (original_feature * predicted_class_selector.unsqueeze(-1)).sum(1) # [N, d]\n",
    "        return original_feature\n",
    "\n",
    "    def get_predicted_class_selector(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Returns the predicted class selector for the given input tensor, with the predicted class set to 1 and all other classes set to 0.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor of shape [N, C, H, W].\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The predicted class selector tensor of shape [N, C], where N is the batch size and C is the number of classes.\n",
    "            \"\"\"\n",
    "            logits = self.pred_model(input_ids, attention_mask).logits # [N, n_classes]\n",
    "            predicted_class_idx = logits.argmax(-1) # [N, 1]\n",
    "            predicted_class_selector = idx_to_selector(predicted_class_idx, self.num_classes) # [N, n_classes]\n",
    "            return predicted_class_selector\n",
    "\n",
    "\n",
    "    def loss_func(self, sim, mask_list, reverse_mask_list, masked_probs_list, reverse_masked_probs_list):\n",
    "        \"\"\"Calculate the loss for the given mask.\n",
    "\n",
    "        Args:\n",
    "            sim (Tensor): The similarity tensor of shape [N, L].\n",
    "            mask (Tensor): The generated mask tensor of shape [N, L].\n",
    "            probs (Tensor): The probability tensor of shape [N,]. Obtained by feeding the randomly generated mask to the prediction model.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The loss tensor of shape [N, L].\n",
    "        \"\"\"\n",
    "        bce_loss = nn.BCELoss(reduction='none')\n",
    "        n_steps = len(mask_list)\n",
    "        L = sim.shape[-1]\n",
    "\n",
    "        # generating probability\n",
    "        mask_prob = torch.sigmoid(sim).unsqueeze(1).expand(-1, n_steps, -1) # [N, n_steps, L]\n",
    "        reverse_mask_prob = 1 - mask_prob # [N, n_steps, L]\n",
    "        # generated mask samples\n",
    "        mask_samples = torch.stack(mask_list, dim=1) # [N, n_steps, L]\n",
    "        reverse_mask_samples = torch.stack(reverse_mask_list, dim=1) # [N, n_steps, L]\n",
    "\n",
    "        # the prediction probability of the masked input\n",
    "        mask_sample_probs = torch.stack(masked_probs_list, dim=1) # [N, n_steps, 1]\n",
    "        # the prediction probability of the reverse masked input\n",
    "        reverse_mask_sample_probs = torch.stack(reverse_masked_probs_list, dim=1) # [N, n_steps, 1]\n",
    "\n",
    "        # reward loss, if mask_sample_probs is higher, we want to optimize the probability of generating the masks\n",
    "        reward_loss = bce_loss(mask_prob , mask_samples) # [N, n_steps, L]\n",
    "        reward_loss = (reward_loss * mask_sample_probs).mean() # [N, n_steps, L]\n",
    "        # regret loss, if reverse_mask_sample_probs is higher, we want to optimize the probability of generating the reverse masks\n",
    "        regret_loss = bce_loss(mask_prob, reverse_mask_samples) * reverse_mask_samples # [N, n_steps, L]\n",
    "        regret_loss = (regret_loss * torch.relu(reverse_mask_sample_probs - 0.1)).mean() \n",
    "\n",
    "        # regret_loss = (regret_loss * torch.relu(0.1 - reverse_mask_sample_probs)).mean() # [N, n_steps, L]\n",
    "        \n",
    "        # total_reward_loss = reward_loss.sum() / (mask_selector.sum() + 1e-5)  + regret_loss.sum() / ((1 - mask_selector).sum() + 1e-5) # [1]\n",
    "\n",
    "        # mask_loss\n",
    "        mask_loss = (mask_prob * mask_samples).sum() / mask_samples.sum() # [1]\n",
    "\n",
    "        alt_mask_loss = ((0.1 - mask_prob.mean([-1, -2]) - mask_sample_probs.mean([-1, -2])) ** 2).mean()\n",
    "        # alt_mask_loss_pos = torch.relu(0.1 - mask_prob.mean([-1, -2]) - mask_sample_probs.mean([-1, -2]))\n",
    "        # alt_mask_loss_neg = torch.relu(mask_prob.mean([-1, -2]) + mask_sample_probs.mean([-1, -2]) - 0.1)\n",
    "        # alt_mask_loss = (alt_mask_loss_pos + alt_mask_loss_neg).mean()\n",
    "        \n",
    "\n",
    "        loss =  reward_loss + 0.01 * alt_mask_loss\n",
    "        mask_mean = mask_prob.mean([1, 2]) # [N]\n",
    "        prob_mean = mask_sample_probs.mean([1, 2]) # [N]\n",
    "\n",
    "        # print(\"mask_prob: \", mask_prob[0, 0])\n",
    "        # print(\"mask_samples: \", mask_samples[0, 0])\n",
    "\n",
    "\n",
    "        return {'loss': loss,\n",
    "                'reward_loss': reward_loss,\n",
    "                'regret_loss': regret_loss,\n",
    "                'mask_mean': mask_mean.mean(),\n",
    "                'prob_mean': prob_mean.mean(),\n",
    "                'mask_loss': mask_loss,\n",
    "                'alt_mask_loss': alt_mask_loss}\n",
    "\n",
    "    def generate_mask(self, sim):\n",
    "        \"\"\"Generate a mask based on the similarity tensor. [generate action based on policy]\n",
    "\n",
    "        Args:\n",
    "            sim (Tensor): The similarity tensor of shape [N, L].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The generated mask tensor of shape [N, L].\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            mask_prob = torch.sigmoid(sim)\n",
    "            # scaler = torch.rand(sim.shape[0], 1, device=sim.device) \n",
    "\n",
    "            # mask_prob = (mask_prob - 0) / (1e-5 + mask_prob.max(dim=-1, keepdim=True)[0])\n",
    "            # mask_prob = mask_prob * scaler\n",
    "            # mask_prob = torch.clamp(mask_prob, 0.2, 1.0) # prevent the mask_prob from being too close to 0 or 1\n",
    "\n",
    "            # sample a mask (action) based on the mask probability (policy)\n",
    "            mask = torch.bernoulli(mask_prob) # [N, L]\n",
    "        \n",
    "        return mask # [N, L]\n",
    "    \n",
    "    def get_mask_probs(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, mask: torch.Tensor, predicted_class_selector: torch.Tensor):\n",
    "        # No gradients upon the parameters of the prediction model\n",
    "        with torch.no_grad():\n",
    "            masked_input_ids = input_ids * mask + (1 - mask) * self.mask_token  # [N, L]\n",
    "            masked_input_ids = masked_input_ids.long()\n",
    "\n",
    "            masked_probs = torch.softmax(self.pred_model(masked_input_ids, attention_mask).logits, dim=-1) # [N, n_classes]\n",
    "            masked_probs = (masked_probs * predicted_class_selector).sum(-1, keepdim=True) # [N, 1]\n",
    "\n",
    "        return masked_probs\n",
    "\n",
    "\n",
    "    def sample_one_step(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, sim: torch.Tensor, predicted_class_selector: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            mask = self.generate_mask(sim)\n",
    "            mask_probs = self.get_mask_probs(input_ids, attention_mask, mask, predicted_class_selector)\n",
    "            # reverse_mask = 1.0 - mask\n",
    "            # reverse_mask_probs = self.get_mask_probs(x, reverse_mask, predicted_class_selector)\n",
    "        return mask, mask_probs #, reverse_mask, reverse_mask_probs\n",
    "\n",
    "\n",
    "    def train_one_batch(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, optimizer: torch.optim.Optimizer, n_steps=10):\n",
    "        self.train()\n",
    "        optimizer.zero_grad()\n",
    "        predicted_class_selector = self.get_predicted_class_selector(input_ids, attention_mask)\n",
    "        outputs = self.forward(input_ids, attention_mask, predicted_class_selector)\n",
    "        sim = outputs['sim']\n",
    "        \n",
    "\n",
    "        mask_list, reverse_mask_list = [], []\n",
    "        masked_probs_list, reverse_masked_probs_list = [], []\n",
    "        for idx in range(n_steps):\n",
    "\n",
    "            mask, masked_probs = self.sample_one_step(input_ids, attention_mask, sim, predicted_class_selector)\n",
    "            reverse_mask, reverse_masked_probs = self.sample_one_step(input_ids, attention_mask, -sim, predicted_class_selector)\n",
    "\n",
    "            mask_list.append(mask)\n",
    "            reverse_mask_list.append(reverse_mask)\n",
    "            masked_probs_list.append(masked_probs)\n",
    "            reverse_masked_probs_list.append(reverse_masked_probs)\n",
    "        \n",
    "        loss_dict = self.loss_func(sim, mask_list, reverse_mask_list, masked_probs_list, reverse_masked_probs_list)\n",
    "        # loss_dict = self.loss_func(sim, mask_list, masked_probs_list)\n",
    "\n",
    "        loss = loss_dict['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss_dict\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, predicted_class_selector: torch.Tensor):\n",
    "        original_feature = self.get_original_text_feature(input_ids, attention_mask, predicted_class_selector) # [N, d]\n",
    "\n",
    "        interpretable_features = self.get_interpretable_text_features(input_ids, attention_mask) # [N, L, d]\n",
    "\n",
    "        \n",
    "        sim = self.similarity_measure(pred_feature=original_feature, explain_features=interpretable_features) # [N, L]\n",
    "        return {'sim': sim}\n",
    "\n",
    "    \n",
    "    def attribute_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            predicted_class_selector = self.get_predicted_class_selector(input_ids, attention_mask)\n",
    "            outputs = self.forward(input_ids, attention_mask, predicted_class_selector)\n",
    "            probs = torch.sigmoid(outputs['sim'])\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from maskgen.models.mask_generating_model12 import MaskGeneratingModel\n",
    "\n",
    "pred_hidden_dim = pred_model.config.dim\n",
    "num_labels = pred_model.config.num_labels\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(pred_model, hidden_size=pred_hidden_dim, num_classes=num_labels)\n",
    "mask_gen_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "# texts = imdb[\"test\"][0]['text']\n",
    "# print(texts)\n",
    "\n",
    "# inputs = tokenizer(texts, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     inputs = {key:val.to(device) for key,val in inputs.items()}\n",
    "#     logits = pred_model(**inputs).logits\n",
    "\n",
    "# predicted_class_id = logits.argmax().item()\n",
    "# pred_model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045,  103, 2023, 3185,  999,  102]], device='cuda:0') tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4772, 0.4838, 0.4667, 0.4982, 0.4786]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen_model.attribute_text(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_ds, val_ds, test_ds\n\u001b[1;32m     12\u001b[0m train_ds, val_ds, test_ds \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m---> 14\u001b[0m normalize \u001b[38;5;241m=\u001b[39m \u001b[43mNormalize\u001b[49m(mean\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mimage_std)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m processor\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m     16\u001b[0m     size \u001b[38;5;241m=\u001b[39m (processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Normalize' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def load_data(seed=42): \n",
    "    dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "    dataset = dataset['train']\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    test_ds = splits['test']\n",
    "    splits = splits['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "    train_ds = splits['train']\n",
    "    val_ds = splits['test']\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = load_data()\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "if \"height\" in processor.size:\n",
    "    size = (processor.size[\"height\"], processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in processor.size:\n",
    "    size = processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = processor.size.get(\"longest_edge\")\n",
    "\n",
    "transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_ds.set_transform(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params to be optimized: \n",
      "['similarity_measure.logit_scale', 'similarity_measure.pred_map.input_layer.weight', 'similarity_measure.pred_map.input_layer.bias', 'similarity_measure.pred_map.layers.0.0.weight', 'similarity_measure.pred_map.layers.0.0.bias', 'similarity_measure.pred_map.layers.0.3.weight', 'similarity_measure.pred_map.layers.0.3.bias', 'similarity_measure.pred_map.layers.0.6.weight', 'similarity_measure.pred_map.layers.0.6.bias', 'similarity_measure.pred_map.layers.1.0.weight', 'similarity_measure.pred_map.layers.1.0.bias', 'similarity_measure.pred_map.layers.1.3.weight', 'similarity_measure.pred_map.layers.1.3.bias', 'similarity_measure.pred_map.layers.1.6.weight', 'similarity_measure.pred_map.layers.1.6.bias', 'similarity_measure.pred_map.output_layer.weight', 'similarity_measure.pred_map.output_layer.bias', 'similarity_measure.explain_map.input_layer.weight', 'similarity_measure.explain_map.input_layer.bias', 'similarity_measure.explain_map.layers.0.0.weight', 'similarity_measure.explain_map.layers.0.0.bias', 'similarity_measure.explain_map.layers.0.3.weight', 'similarity_measure.explain_map.layers.0.3.bias', 'similarity_measure.explain_map.layers.0.6.weight', 'similarity_measure.explain_map.layers.0.6.bias', 'similarity_measure.explain_map.layers.1.0.weight', 'similarity_measure.explain_map.layers.1.0.bias', 'similarity_measure.explain_map.layers.1.3.weight', 'similarity_measure.explain_map.layers.1.3.bias', 'similarity_measure.explain_map.layers.1.6.weight', 'similarity_measure.explain_map.layers.1.6.bias', 'similarity_measure.explain_map.output_layer.weight', 'similarity_measure.explain_map.output_layer.bias']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_workers = 8\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=num_workers)\n",
    "n_steps = 10\n",
    "\n",
    "params_to_optimize = [name for name, param in mask_gen_model.named_parameters() if param.requires_grad]\n",
    "print(\"params to be optimized: \")\n",
    "print(params_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = 0.4140, Reward Loss = 0.4030, Regret Loss = 0.1692, Mask Loss = 0.5087 alt_mask_loss = 1.1055mask_mean = 0.5072 prob_mean = 0.5827 :   0%|          | 0/159 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.1257, Reward Loss = 0.1238, Regret Loss = 0.8926, Mask Loss = 0.1967 alt_mask_loss = 0.1860mask_mean = 0.1910 prob_mean = 0.2486 :   7%|▋         | 11/159 [02:51<37:28, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 21: Loss = 0.1666, Reward Loss = 0.1637, Regret Loss = 0.7182, Mask Loss = 0.2436 alt_mask_loss = 0.2834mask_mean = 0.2319 prob_mean = 0.2978 :  13%|█▎        | 21/159 [05:21<34:32, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 31: Loss = 0.2207, Reward Loss = 0.2165, Regret Loss = 0.6554, Mask Loss = 0.2860 alt_mask_loss = 0.4217mask_mean = 0.2576 prob_mean = 0.3772 :  19%|█▉        | 30/159 [07:51<32:15, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 41: Loss = 0.1214, Reward Loss = 0.1194, Regret Loss = 0.9819, Mask Loss = 0.2219 alt_mask_loss = 0.2056mask_mean = 0.1826 prob_mean = 0.2432 :  26%|██▌       | 41/159 [10:53<36:15, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 51: Loss = 0.1972, Reward Loss = 0.1934, Regret Loss = 0.7126, Mask Loss = 0.3066 alt_mask_loss = 0.3843mask_mean = 0.2377 prob_mean = 0.3591 :  32%|███▏      | 51/159 [13:23<27:12, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 61: Loss = 0.1737, Reward Loss = 0.1703, Regret Loss = 0.7753, Mask Loss = 0.2869 alt_mask_loss = 0.3375mask_mean = 0.2195 prob_mean = 0.3282 :  38%|███▊      | 60/159 [15:53<24:47, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 71: Loss = 0.1025, Reward Loss = 0.1009, Regret Loss = 0.9814, Mask Loss = 0.2353 alt_mask_loss = 0.1653mask_mean = 0.1765 prob_mean = 0.2131 :  45%|████▍     | 71/159 [18:56<22:44, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 81: Loss = 0.2228, Reward Loss = 0.2184, Regret Loss = 0.7646, Mask Loss = 0.3091 alt_mask_loss = 0.4400mask_mean = 0.2400 prob_mean = 0.4097 :  51%|█████     | 81/159 [21:26<19:31, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 91: Loss = 0.1615, Reward Loss = 0.1586, Regret Loss = 0.8333, Mask Loss = 0.2764 alt_mask_loss = 0.2930mask_mean = 0.2117 prob_mean = 0.3126 :  57%|█████▋    | 90/159 [24:23<23:02, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 101: Loss = 0.1987, Reward Loss = 0.1947, Regret Loss = 0.8592, Mask Loss = 0.3046 alt_mask_loss = 0.3957mask_mean = 0.2211 prob_mean = 0.3818 :  64%|██████▎   | 101/159 [26:58<14:42, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 111: Loss = 0.1892, Reward Loss = 0.1855, Regret Loss = 0.7670, Mask Loss = 0.3096 alt_mask_loss = 0.3671mask_mean = 0.2307 prob_mean = 0.3551 :  70%|██████▉   | 111/159 [29:55<15:37, 19.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 121: Loss = 0.1331, Reward Loss = 0.1308, Regret Loss = 0.8926, Mask Loss = 0.2541 alt_mask_loss = 0.2290mask_mean = 0.1885 prob_mean = 0.2702 :  75%|███████▌  | 120/159 [32:52<11:56, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 131: Loss = 0.1791, Reward Loss = 0.1757, Regret Loss = 0.8903, Mask Loss = 0.2871 alt_mask_loss = 0.3425mask_mean = 0.2129 prob_mean = 0.3482 :  82%|████████▏ | 131/159 [35:26<07:02, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 141: Loss = 0.1965, Reward Loss = 0.1925, Regret Loss = 0.7628, Mask Loss = 0.3267 alt_mask_loss = 0.4003mask_mean = 0.2316 prob_mean = 0.3712 :  89%|████████▊ | 141/159 [37:56<04:30, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 151: Loss = 0.1660, Reward Loss = 0.1628, Regret Loss = 0.9118, Mask Loss = 0.2874 alt_mask_loss = 0.3131mask_mean = 0.2006 prob_mean = 0.3332 :  94%|█████████▍| 150/159 [41:01<02:35, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 159: Loss = 0.1528, Reward Loss = 0.1498, Regret Loss = 0.8862, Mask Loss = 0.2899 alt_mask_loss = 0.3010mask_mean = 0.2032 prob_mean = 0.3029 : 100%|██████████| 159/159 [43:14<00:00, 16.32s/it]\n",
      "Epoch 2, Step 1: Loss = 0.1720, Reward Loss = 0.1686, Regret Loss = 0.9114, Mask Loss = 0.2959 alt_mask_loss = 0.3338mask_mean = 0.2090 prob_mean = 0.3375 :   0%|          | 0/159 [00:17<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 11: Loss = 0.1928, Reward Loss = 0.1888, Regret Loss = 0.8265, Mask Loss = 0.3369 alt_mask_loss = 0.4017mask_mean = 0.2268 prob_mean = 0.3725 :   7%|▋         | 11/159 [02:51<37:03, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 21: Loss = 0.2474, Reward Loss = 0.2417, Regret Loss = 0.7374, Mask Loss = 0.3941 alt_mask_loss = 0.5681mask_mean = 0.2574 prob_mean = 0.4780 :  13%|█▎        | 21/159 [05:59<47:45, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 31: Loss = 0.1648, Reward Loss = 0.1614, Regret Loss = 0.9199, Mask Loss = 0.3176 alt_mask_loss = 0.3357mask_mean = 0.2001 prob_mean = 0.3411 :  19%|█▉        | 30/159 [08:28<32:41, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 41: Loss = 0.1194, Reward Loss = 0.1173, Regret Loss = 1.0752, Mask Loss = 0.2424 alt_mask_loss = 0.2096mask_mean = 0.1667 prob_mean = 0.2626 :  26%|██▌       | 41/159 [11:02<29:31, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 50: Loss = 0.2399, Reward Loss = 0.2347, Regret Loss = 0.7584, Mask Loss = 0.3739 alt_mask_loss = 0.5208mask_mean = 0.2553 prob_mean = 0.4573 :  31%|███▏      | 50/159 [13:17<27:11, 14.96s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "params_to_optimize = [param for param in mask_gen_model.parameters() if param.requires_grad]\n",
    "# optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        loss_dict = mask_gen_model.train_one_batch(pixel_values, optimizer=optimizer, n_steps=n_steps)\n",
    "        # scheduler.step()\n",
    "        pbar.set_description(f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss'].item():.4f}, \" \n",
    "                             f\"Reward Loss = {loss_dict['reward_loss'].item():.4f}, \"\n",
    "                             f\"Regret Loss = {loss_dict['regret_loss'].item():.4f}, \"\n",
    "                             f\"Mask Loss = {loss_dict['mask_loss'].item():.4f} \"\n",
    "                             f\"alt_mask_loss = {loss_dict['alt_mask_loss'].item():.4f}\"\n",
    "                             f\"mask_mean = {loss_dict['mask_mean'].item():.4f} \"\n",
    "                             f\"prob_mean = {loss_dict['prob_mean'].item():.4f} \"\n",
    "                             )\n",
    "        if idx % 10 == 0:\n",
    "            print()\n",
    "        if (idx) % 30 == 0:\n",
    "            \n",
    "            torch.save(mask_gen_model.state_dict(), f'mask_gen_model/mask_gen_model_{epoch}_{idx}.pth') \n",
    "\n",
    "\n",
    "\n",
    "torch.save(mask_gen_model.state_dict(), f'mask_gen_model/mask_gen_model_final_{epoch}_{idx}.pth') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
